{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist=MNIST('./Mnist1/')\n",
    "mnist.gz= True\n",
    "x_train,y_train=mnist.load_training()\n",
    "x_val,y_val=mnist.load_testing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(x_train).astype(np.float32)\n",
    "y_train = np.asarray(y_train).astype(np.int32)\n",
    "x_val= np.asarray(x_val).astype(np.float32)\n",
    "y_val = np.asarray(y_val).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(x_train==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gzip\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 , Time:5.18s,Train_Accuracy:9.38%\n",
      "Epoch: 1, Time Spent: 6.03s,Test_Accuracy: 14.06%\n",
      "Epoch:2 , Time:6.15s,Train_Accuracy:9.38%\n",
      "Epoch: 2, Time Spent: 6.18s,Test_Accuracy: 7.81%\n",
      "Epoch:3 , Time:6.44s,Train_Accuracy:7.81%\n",
      "Epoch: 3, Time Spent: 6.45s,Test_Accuracy: 1.56%\n",
      "Epoch:4 , Time:6.53s,Train_Accuracy:10.94%\n",
      "Epoch: 4, Time Spent: 6.54s,Test_Accuracy: 4.69%\n",
      "Epoch:5 , Time:6.61s,Train_Accuracy:10.94%\n",
      "Epoch: 5, Time Spent: 6.63s,Test_Accuracy: 9.38%\n",
      "Epoch:6 , Time:6.70s,Train_Accuracy:9.38%\n",
      "Epoch: 6, Time Spent: 6.72s,Test_Accuracy: 7.81%\n",
      "Epoch:7 , Time:6.88s,Train_Accuracy:7.81%\n",
      "Epoch: 7, Time Spent: 6.91s,Test_Accuracy: 6.25%\n",
      "Epoch:8 , Time:7.00s,Train_Accuracy:10.94%\n",
      "Epoch: 8, Time Spent: 7.04s,Test_Accuracy: 12.50%\n",
      "Epoch:9 , Time:7.13s,Train_Accuracy:9.38%\n",
      "Epoch: 9, Time Spent: 7.15s,Test_Accuracy: 7.81%\n",
      "Epoch:10 , Time:7.25s,Train_Accuracy:9.38%\n",
      "Epoch: 10, Time Spent: 7.28s,Test_Accuracy: 9.38%\n",
      "Epoch:11 , Time:7.38s,Train_Accuracy:14.06%\n",
      "Epoch: 11, Time Spent: 7.39s,Test_Accuracy: 7.81%\n",
      "Epoch:12 , Time:7.49s,Train_Accuracy:4.69%\n",
      "Epoch: 12, Time Spent: 7.51s,Test_Accuracy: 6.25%\n",
      "Epoch:13 , Time:7.61s,Train_Accuracy:15.62%\n",
      "Epoch: 13, Time Spent: 7.63s,Test_Accuracy: 17.19%\n",
      "Epoch:14 , Time:7.70s,Train_Accuracy:15.62%\n",
      "Epoch: 14, Time Spent: 7.72s,Test_Accuracy: 9.38%\n",
      "Epoch:15 , Time:7.79s,Train_Accuracy:4.69%\n",
      "Epoch: 15, Time Spent: 7.81s,Test_Accuracy: 7.81%\n",
      "Epoch:16 , Time:7.88s,Train_Accuracy:9.38%\n",
      "Epoch: 16, Time Spent: 7.90s,Test_Accuracy: 14.06%\n",
      "Epoch:17 , Time:7.97s,Train_Accuracy:6.25%\n",
      "Epoch: 17, Time Spent: 7.99s,Test_Accuracy: 17.19%\n",
      "Epoch:18 , Time:8.08s,Train_Accuracy:4.69%\n",
      "Epoch: 18, Time Spent: 8.10s,Test_Accuracy: 7.81%\n",
      "Epoch:19 , Time:8.17s,Train_Accuracy:14.06%\n",
      "Epoch: 19, Time Spent: 8.19s,Test_Accuracy: 7.81%\n",
      "Epoch:20 , Time:8.27s,Train_Accuracy:7.81%\n",
      "Epoch: 20, Time Spent: 8.29s,Test_Accuracy: 9.38%\n",
      "Epoch:21 , Time:8.36s,Train_Accuracy:9.38%\n",
      "Epoch: 21, Time Spent: 8.38s,Test_Accuracy: 4.69%\n",
      "Epoch:22 , Time:8.45s,Train_Accuracy:9.38%\n",
      "Epoch: 22, Time Spent: 8.47s,Test_Accuracy: 6.25%\n",
      "Epoch:23 , Time:8.54s,Train_Accuracy:9.38%\n",
      "Epoch: 23, Time Spent: 8.56s,Test_Accuracy: 15.62%\n",
      "Epoch:24 , Time:8.63s,Train_Accuracy:7.81%\n",
      "Epoch: 24, Time Spent: 8.65s,Test_Accuracy: 7.81%\n",
      "Epoch:25 , Time:8.73s,Train_Accuracy:10.94%\n",
      "Epoch: 25, Time Spent: 8.75s,Test_Accuracy: 9.38%\n",
      "Epoch:26 , Time:8.82s,Train_Accuracy:9.38%\n",
      "Epoch: 26, Time Spent: 8.84s,Test_Accuracy: 4.69%\n",
      "Epoch:27 , Time:8.91s,Train_Accuracy:9.38%\n",
      "Epoch: 27, Time Spent: 8.93s,Test_Accuracy: 9.38%\n",
      "Epoch:28 , Time:9.00s,Train_Accuracy:4.69%\n",
      "Epoch: 28, Time Spent: 9.03s,Test_Accuracy: 7.81%\n",
      "Epoch:29 , Time:9.11s,Train_Accuracy:10.94%\n",
      "Epoch: 29, Time Spent: 9.13s,Test_Accuracy: 7.81%\n",
      "Epoch:30 , Time:9.20s,Train_Accuracy:10.94%\n",
      "Epoch: 30, Time Spent: 9.22s,Test_Accuracy: 14.06%\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.datasets import fetch_openml\n",
    "#from keras.utils.np_utils import to_categorical\n",
    "#from mnist import MNIST\n",
    "#from keras.utils.np_utils import to_categorical\n",
    "#from sklearn.datasets import fetch_openml\n",
    "#import numpy as np\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#import gzip\n",
    "#import time\n",
    "\n",
    "'''\n",
    "mnist=MNIST('./Mnist1/')\n",
    "mnist.gz= True\n",
    "x_train,y_train=mnist.load_training()\n",
    "x_val,y_val=mnist.load_testing()\n",
    "\n",
    "x_train = np.asarray(x_train).astype(np.float32)\n",
    "y_train = np.asarray(y_train).astype(np.int32)\n",
    "x_val= np.asarray(x_val).astype(np.float32)\n",
    "y_val = np.asarray(y_val).astype(np.int32)\n",
    "start_index=0\n",
    "\n",
    "'''\n",
    "\n",
    "x, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "x = (x/255).astype('float32')\n",
    "y = to_categorical(y)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.15, random_state=42)\n",
    "start_index=0\n",
    "\n",
    "\n",
    "class DeepNeuralNetwork():\n",
    "    def __init__(self, sizes, epochs=30, l_rate=0.001,B_size=64):\n",
    "        self.sizes = sizes\n",
    "        self.epochs = epochs\n",
    "        self.l_rate = l_rate\n",
    "        self.B_size=B_size\n",
    "\n",
    "        # we save all parameters in the neural network in this dictionary\n",
    "        self.params = self.initialization()\n",
    "\n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        if derivative:\n",
    "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x, derivative=False):\n",
    "        # Numerically stable with large exponentials\n",
    "        exps = np.exp(x - x.max())\n",
    "        if derivative:\n",
    "            return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "\n",
    "    def initialization(self):\n",
    "        # number of nodes in each layer\n",
    "        input_layer=self.sizes[0]\n",
    "        hidden_1=self.sizes[1]\n",
    "        hidden_2=self.sizes[2]\n",
    "        hidden_3=self.sizes[3]\n",
    "        output_layer=self.sizes[4]\n",
    "        #np.random.seed(43)\n",
    "\n",
    "        params = {\n",
    "            \n",
    "            'W1':np.random.randn(hidden_1, input_layer) * np.sqrt(1. /hidden_1),\n",
    "            'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
    "            'W3':np.random.randn(hidden_3,hidden_2) * np.sqrt(1./hidden_3),\n",
    "            'W4':np.random.randn(output_layer, hidden_3) * np.sqrt(1. / output_layer)\n",
    "            \n",
    "            }\n",
    "\n",
    "        return params\n",
    "\n",
    "    def forward_pass(self, x_train):\n",
    "        params = self.params\n",
    "\n",
    "        # input layer activations becomes sample\n",
    "        params['A0'] = x_train\n",
    "\n",
    "        # input layer to hidden layer 1\n",
    "        params['Z1'] = np.dot(params[\"W1\"], params['A0'])\n",
    "        params['A1'] = self.sigmoid(params['Z1'])\n",
    "\n",
    "        # hidden layer 1 to hidden layer 2\n",
    "        params['Z2'] = np.dot(params[\"W2\"], params['A1'])\n",
    "        params['A2'] = self.sigmoid(params['Z2'])\n",
    "        \n",
    "        #hidden layer 2 to  hidden layer 3\n",
    "        params['Z3']=np.dot(params[\"W3\"],params[\"A2\"])\n",
    "        params[\"A3\"]=self.sigmoid(params['Z3'])\n",
    "\n",
    "        # hidden layer 3 to output layer\n",
    "        params['Z4'] = np.dot(params[\"W4\"], params['A3'])\n",
    "        params['A4'] = self.softmax(params['Z4'])\n",
    "\n",
    "        return params['A4']\n",
    "\n",
    "    def backward_pass(self, y_train, output):\n",
    "        '''\n",
    "            This is the backpropagation algorithm, for calculating the updates\n",
    "            of the neural network's parameters.\n",
    "\n",
    "            Note: There is a stability issue that causes warnings. This is \n",
    "                  caused  by the dot and multiply operations on the huge arrays.\n",
    "                  \n",
    "                  RuntimeWarning: invalid value encountered in true_divide\n",
    "                  RuntimeWarning: overflow encountered in exp\n",
    "                  RuntimeWarning: overflow encountered in square\n",
    "        '''\n",
    "        params = self.params\n",
    "        change_w = {}\n",
    "\n",
    "        # Calculate W4 update\n",
    "        error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z4'], derivative=True)\n",
    "        change_w['W4'] = np.outer(error, params['A3'])\n",
    "        \n",
    "        #claculate W3 update \n",
    "        error=np.dot(params[\"W4\"].T,error)*self.sigmoid(params['Z3'])\n",
    "        change_w[\"W3\"]=np.outer(error, params[\"A2\"])\n",
    "\n",
    "        # Calculate W2 update\n",
    "        error = np.dot(params['W3'].T, error) * self.sigmoid(params['Z2'], derivative=True)\n",
    "        change_w['W2'] = np.outer(error, params['A1'])\n",
    "\n",
    "        # Calculate W1 update\n",
    "        error = np.dot(params['W2'].T, error) * self.sigmoid(params['Z1'], derivative=True)\n",
    "        change_w['W1'] = np.outer(error, params['A0'])\n",
    "\n",
    "        return change_w\n",
    "\n",
    "    def update_network_parameters(self, changes_to_w):\n",
    "        '''\n",
    "            Update network parameters according to update rule from\n",
    "            Stochastic Gradient Descent.\n",
    "\n",
    "            θ = θ - η * ∇J(x, y), \n",
    "                theta θ:            a network parameter (e.g. a weight w)\n",
    "                eta η:              the learning rate\n",
    "                gradient ∇J(x, y):  the gradient of the objective function,\n",
    "                                    i.e. the change for a specific theta θ\n",
    "        '''\n",
    "        \n",
    "        for key, value in changes_to_w.items():\n",
    "            self.params[key] -= self.l_rate * value\n",
    "\n",
    "    def compute_accuracy(self, x_val, y_val):\n",
    "        '''\n",
    "            This function does a forward pass of x, then checks if the indices\n",
    "            of the maximum value in the output equals the indices in the label\n",
    "            y. Then it sums over each prediction and calculates the accuracy.\n",
    "        '''\n",
    "        predictions = []\n",
    "\n",
    "        for x, y in zip(x_val, y_val):\n",
    "            output = self.forward_pass(x)\n",
    "            pred = np.argmax(output)\n",
    "            predictions.append(pred== np.argmax(y))\n",
    "            \n",
    "        \n",
    "        return np.mean(predictions)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def update_batchSize(x_train2,Batch_size):\n",
    "        global start_index\n",
    "        #print(Batch_size)\n",
    "        finish_index=start_index+Batch_size\n",
    "        #print(\"here:\",len(x_train2),n,s)\\\n",
    "        #print(\"here1:\",finish_index,start_index)\n",
    "        x_train3=x_train2[(start_index):(finish_index)]\n",
    "        start_index=finish_index\n",
    "        #print(len(x_train3))\n",
    "        return (x_train3)\n",
    "    def up():\n",
    "        return 1   \n",
    "    \n",
    "    def  train(self, x_train, y_train, x_val, y_val):\n",
    "        #update_batchSize(x_train,64)\n",
    "        start_time = time.time()\n",
    "        Batch=self.B_size \n",
    "        #print(Batch)\n",
    "        for iteration in range(self.epochs):\n",
    "            x_train1=dnn.update_batchSize(x_train,Batch)\n",
    "            y_train1=dnn.update_batchSize(y_train,Batch)\n",
    "            x_val1=dnn.update_batchSize(x_val,Batch)\n",
    "            y_val1=dnn.update_batchSize(y_val,Batch)\n",
    "            \n",
    "            for x,y in zip(x_train1, y_train1):\n",
    "                output = self.forward_pass(x)\n",
    "                changes_to_w = self.backward_pass(y, output)\n",
    "                self.update_network_parameters(changes_to_w)\n",
    "            \n",
    "                       \n",
    "            train_accuracy=self.compute_accuracy(x_train1,y_train1)\n",
    "            print('Epoch:{0} , Time:{1:.2f}s,Train_Accuracy:{2:.2f}%'.format(\n",
    "                iteration+1,time.time()- start_time, train_accuracy*100 ))\n",
    "            \n",
    "            test_accuracy = self.compute_accuracy(x_val1, y_val1)\n",
    "            print('Epoch: {0}, Time Spent: {1:.2f}s,Test_Accuracy: {2:.2f}%'.format(\n",
    "                iteration+1, time.time() - start_time, test_accuracy * 100\n",
    "            ))\n",
    "            \n",
    "#update_batchSize(x_train,ytrain)\n",
    "#for x, y in zip(x_val, y_val):\n",
    "    #output = self.forward_pass(x)\n",
    "    #print(output)\n",
    "            \n",
    "dnn = DeepNeuralNetwork(sizes=[784, 64, 35,16, 10])\n",
    "#dnn.update_batchSize(x_train,64)\n",
    "dnn.train(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train1[0:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hidden_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-6e8a0084b326>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m params = {\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m             \u001b[1;34m'W1'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_layer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0minput_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m             \u001b[1;34m'W2'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mhidden_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[1;34m'W3'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhidden_2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mhidden_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hidden_1' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "params = {\n",
    "            \n",
    "            'W1':np.random.randn(hidden_1, input_layer) * np.sqrt(1. / input_layer),\n",
    "            'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_1),\n",
    "            'W3':np.random.randn(hidden_3,hidden_2) * np.sqrt(1./hidden_2),\n",
    "            'W4':np.random.randn(output_layer, hidden_3) * np.sqrt(1. / hidden_3)\n",
    "            \n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params['w1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
